{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Experimento_TS_Astro_GAN_v5_2D_dift",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rii0JUCTb-7e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "from pandas_datareader import data\n",
        "import tensorflow as tf\n",
        "#from tensorflow.contrib.rnn import LSTMCell\n",
        "tf.logging.set_verbosity(tf.logging.ERROR)\n",
        "from time import time\n",
        "from matplotlib.colors import hsv_to_rgb\n",
        "from scipy.stats import ks_2samp\n",
        "import matplotlib.mlab as mlab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hc4-ChFggIib",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# STEP 2 : DEFINE THE NETWORKS FOR THE GAN\n",
        "\n",
        "# --- to do with training --- #\n",
        "\n",
        "# function for getting one mini batch\n",
        "def get_batch(samples, batch_size, batch_idx):\n",
        "    start_pos = batch_idx * batch_size\n",
        "    end_pos = start_pos + batch_size\n",
        "    return samples[start_pos:end_pos,:,:]\n",
        "\n",
        "\n",
        "\n",
        "# # tf.reset_default_graph()\n",
        "\n",
        "# print(\"loading data\")\n",
        "\n",
        "# samples1 = sample_data(n_samples = 50000)\n",
        "# # print(samples1.shape)\n",
        "# # a=tf.tile(samples1, [1,2])\n",
        "# # print(a.shape)\n",
        "# # print(\"XXXXXXXXXXXXX\")\n",
        "# # # print(a)\n",
        "# # samples2 = tf.reshape(a, [tf.shape(samples1)[0],tf.shape(samples1)[1],2])\n",
        "# samples3=tf.ones([2, 1, 1]) * samples1\n",
        "\n",
        "# sess = tf.Session()\n",
        "# with sess.as_default():\n",
        "# #   a1=a.eval()\n",
        "#   samples=samples3.eval()\n",
        "# #   samples=samples2.eval()\n",
        "# sess.close()\n",
        "# # # print(a.shape)\n",
        "\n",
        "# # print(samples1)\n",
        "# # print(samples)\n",
        "# # print(samples.shape)\n",
        "# # print(\"ssssss\")\n",
        "# # print(samples[1,:,:])\n",
        "# samples=np.transpose(samples, (1, 2, 0))\n",
        "# print(samples.shape)\n",
        "# # print(samples2[:,:,1])\n",
        "# # print(samples2[0,:,:])\n",
        "\n",
        "# # # # samples = tf.tile(samples1,[1,1,2])\n",
        "# # # # samples[:,:,0]=samples1\n",
        "# # # # samples[:,:,1]=samples1\n",
        "# # print(samples1)\n",
        "# # print(samples)\n",
        "# # print(samples.shape)\n",
        "# # # print(samples1)\n",
        "# # # print(a1)\n",
        "# # print(samples[0,:,:])\n",
        "# # print(samples[0,:,:].shape)\n",
        "# print(\"data loaded\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb11OvE7RS5q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jprcyUZ2y3w",
        "colab_type": "code",
        "outputId": "35d9ab7e-a216-485a-cc8e-2e12a26afa85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive,files\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "# uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDRoHZ343wT9",
        "colab_type": "code",
        "outputId": "684db11e-3717-496e-8e77-1174e2436dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print(uploaded)\n",
        "# print(type(uploaded))\n",
        "# aa=np.load(uploaded)\n",
        "samples = np.load('/content/gdrive/My Drive/Work/GAN_RNN/data_tensor.npy', encoding='bytes')\n",
        "#samples = np.load('data_tensor.npy', encoding='bytes')\n",
        "\n",
        "print(samples.shape)\n",
        "seq_length=360"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 360, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DG-9s8cBAQws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# oldsamples=samples\n",
        "# samples = np.delete(samples, (0), axis=2)\n",
        "# # oldsamples1=samples\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# # samples1=np.reshape(samples,(100,360))\n",
        "# X_one_column = samples.reshape([-1,1])\n",
        "\n",
        "# # print(samples1.shape)\n",
        "# print(X_one_column.shape)\n",
        "# # scaler = scaler.fit(samples1)\n",
        "# scaler = scaler.fit(X_one_column)\n",
        "# # samples = scaler.transform(samples1)\n",
        "# samples = scaler.transform(X_one_column)\n",
        "# # samples=np.reshape(samples1,(100,360,1))\n",
        "# samples=np.reshape(samples,(100,360,1))\n",
        "# print(samples.shape)\n",
        "# nsamples=samples\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDND8fgpiBtg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kMPisNaciDmB",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "oldsamples=samples\n",
        "samples = np.delete(samples, (0), axis=2)\n",
        "# print(samples)\n",
        "# print(samples.shape)\n",
        "samples1 = samples.reshape([100,360])\n",
        "# print(samples1)\n",
        "# print(samples1.shape)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaler = scaler.fit(samples1.T)\n",
        "samples2 = scaler.transform(samples1.T).T\n",
        "# print(samples2)\n",
        "# print(samples2.shape)\n",
        "# print(samples2[1,:])\n",
        "samples=np.reshape(samples2,(100,360,1))\n",
        "# print(samples[1,:,0])\n",
        "nsamples=samples\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiRDfw0pVxwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "oldsamples=samples\n",
        "scalers = {}\n",
        "for i in range(samples.shape[2]):\n",
        "    samples1 = samples[:,:,i].reshape([100,360])\n",
        "    print(samples1)\n",
        "    dsamples1=np.diff(samples1)\n",
        "    print('XXXXXXXXXXX')\n",
        "    print(dsamples1)\n",
        "    print(dsamples1.shape)\n",
        "    dsamples2=np.zeros_like(samples1)\n",
        "    dsamples2[:,1:]=dsamples1\n",
        "    print(dsamples2)    \n",
        "    print(dsamples2.shape)\n",
        "    \n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4SsalK9Ejr9N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "oldsamples=samples\n",
        "scalers = {}\n",
        "for i in range(samples.shape[2]):\n",
        "    samples1 = samples[:,:,i].reshape([100,360])\n",
        "    if i==0:\n",
        "      dsamples1=np.diff(samples1)\n",
        "      dsamples2=np.zeros_like(samples1)\n",
        "      dsamples2[:,1:]=dsamples1\n",
        "      samples1=dsamples2          \n",
        "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    scaler = scaler.fit(samples1.T)\n",
        "    samples[:,:,i]=scaler.transform(samples1.T).T\n",
        "    scalers[i]=scaler\n",
        "    #X_train[:, i, :] = scalers[i].fit_transform(X_train[:, i, :]) \n",
        "\n",
        "# samples = np.delete(samples, (0), axis=2)\n",
        "# # print(samples)\n",
        "# # print(samples.shape)\n",
        "# samples1 = samples.reshape([100,360])\n",
        "# # print(samples1)\n",
        "# # print(samples1.shape)\n",
        "# scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# scaler = scaler.fit(samples1.T)\n",
        "# samples2 = scaler.transform(samples1.T).T\n",
        "# # print(samples2)\n",
        "# # print(samples2.shape)\n",
        "# # print(samples2[1,:])\n",
        "# samples=np.reshape(samples2,(100,360,1))\n",
        "# # print(samples[1,:,0])\n",
        "nsamples=samples\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MRWCqvdAxHD",
        "colab_type": "code",
        "outputId": "6e13af6d-09dc-40c4-d517-9fbdf1b6be43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(samples.shape)\n",
        "# print(samples)\n",
        "print(samples[0,:,0])\n",
        "print(samples[-1,:,0].shape)\n",
        "# print(scaler.data_max_)\n",
        "# print(scaler.data_min_)\n",
        "# print(samples[0,:,0])\n",
        "# print(oldsamples)\n",
        "# print(oldsamples[-1,:,1])\n",
        "# print(oldsamples[-1,:,1].shape)\n",
        "\n",
        "# print(oldsamples1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 360, 2)\n",
            "[0.00000000e+00 2.39214175e-02 1.71865925e-02 5.32761573e-02\n",
            " 3.50512209e-02 5.97901083e-02 4.05407065e-02 1.75761339e-02\n",
            " 1.20698170e-02 1.12890807e-02 1.19400092e-02 3.50636229e-02\n",
            " 1.25586925e-02 2.39870301e-02 2.87506436e-02 1.75013675e-02\n",
            " 6.04993854e-03 1.77441517e-02 1.76849763e-02 1.77479313e-02\n",
            " 1.17151194e-02 2.99859432e-02 1.71547607e-02 2.36864289e-02\n",
            " 1.76845629e-02 1.20914910e-02 1.20218625e-02 2.94431484e-02\n",
            " 1.20349732e-02 1.16351559e-02 1.16384631e-02 1.19232370e-02\n",
            " 1.19387100e-02 1.15259001e-02 1.16557078e-02 1.18389033e-02\n",
            " 6.45100785e-02 4.12093523e-02 1.77980118e-02 2.95799840e-02\n",
            " 4.10555673e-02 3.54108202e-02 5.90229552e-02 1.00425129e-01\n",
            " 1.94762287e-01 6.81673202e-01 4.68304764e-02 8.27284005e-02\n",
            " 2.31513115e-02 3.61302552e-02 2.34180729e-02 2.51583708e-05\n",
            " 2.36631013e-02 2.37463129e-02 5.27091490e-02 3.50722453e-02\n",
            " 1.78869520e-02 4.74652823e-02 2.94644681e-02 3.49611587e-02\n",
            " 4.69990257e-02 5.89939581e-02 1.73835483e-02 1.28495493e-02\n",
            " 1.17797870e-02 1.07085483e-02 1.76972012e-02 1.28989211e-02\n",
            " 3.43001904e-02 1.79512653e-02 1.18009885e-02 1.17536837e-02\n",
            " 2.96659712e-02 1.20998771e-02 1.74374085e-02 1.22736234e-02\n",
            " 1.17276395e-02 1.78075791e-02 1.12426617e-02 1.19857786e-02\n",
            " 1.15807642e-02 1.20603678e-02 1.13881196e-02 1.18530770e-02\n",
            " 1.18089613e-02 1.17987444e-02 1.19703646e-02 1.19383556e-02\n",
            " 2.32664731e-02 5.28678358e-02 2.39878569e-02 3.53312702e-02\n",
            " 4.10754105e-02 1.17843108e-01 8.76366923e-01 2.92091047e-02\n",
            " 2.38160595e-02 2.94776969e-02 2.33962218e-02 3.57057519e-02\n",
            " 1.74140809e-02 2.34642557e-02 2.95521090e-02 3.54451915e-02\n",
            " 1.82470828e-02 2.95151982e-02 1.75288881e-02 5.30124078e-02\n",
            " 1.10715139e-02 1.78750815e-02 1.76200725e-02 1.76641882e-02\n",
            " 1.80813093e-02 1.14130417e-02 1.78408283e-02 1.76388527e-02\n",
            " 1.75134151e-02 1.27740742e-02 1.77605696e-02 2.24315223e-02\n",
            " 4.74576049e-02 1.28807315e-02 1.63215226e-02 1.29415604e-02\n",
            " 2.31881632e-02 1.70258388e-02 1.19271938e-02 1.16809252e-02\n",
            " 1.19825895e-02 1.19851289e-02 1.15316286e-02 1.17983310e-02\n",
            " 1.20018421e-02 1.16665743e-02 1.77199973e-02 1.17236236e-02\n",
            " 1.20544030e-02 1.20003066e-02 1.17054930e-02 1.16288368e-02\n",
            " 1.18432145e-02 1.20200317e-02 1.24241602e-02 1.66814172e-02\n",
            " 1.18512462e-02 1.75476683e-02 1.17409274e-02 1.77695463e-02\n",
            " 1.15269040e-02 1.18441003e-02 1.17735860e-02 1.17672078e-02\n",
            " 1.18277415e-02 1.17841572e-02 1.18265603e-02 5.86798328e-03\n",
            " 5.91227619e-03 1.77906297e-02 1.77029888e-02 1.17187809e-02\n",
            " 1.76842676e-02 1.77480494e-02 1.23145500e-02 2.35709721e-02\n",
            " 1.18533723e-02 1.73383696e-02 1.76424551e-02 1.76146392e-02\n",
            " 3.54170213e-02 1.78440764e-02 1.75485542e-02 2.36054615e-02\n",
            " 2.35951855e-02 1.78748453e-02 4.71209788e-02 5.31522553e-02\n",
            " 5.80959342e-01 9.45850209e-02 2.97107366e-02 1.74626259e-02\n",
            " 4.15342260e-02 6.48428068e-02 4.72364357e-02 1.73645910e-02\n",
            " 3.49203501e-02 2.95679363e-02 1.15069427e-02 1.22903366e-02\n",
            " 2.93110965e-02 1.77258439e-02 1.19094766e-02 1.20033185e-02\n",
            " 2.31575125e-02 4.08708363e-02 4.25759951e-02 4.13636097e-02\n",
            " 5.10372395e-03 2.43387748e-02 5.21048166e-02 2.35804803e-02\n",
            " 1.76796612e-02 2.37010751e-02 5.32613930e-02 4.70155026e-02\n",
            " 2.36144382e-02 2.95984098e-02 2.36406596e-02 4.71130061e-02\n",
            " 1.17258087e-02 1.77686604e-02 1.73008092e-02 2.37618450e-02\n",
            " 2.93865125e-02 3.58445364e-02 4.69627645e-02 2.94582080e-02\n",
            " 4.70789301e-02 4.72561608e-02 7.05201062e-01 1.35549876e-01\n",
            " 9.45317513e-02 3.53330419e-02 4.15318637e-02 4.12186242e-02\n",
            " 2.87671205e-02 3.54678695e-02 1.17753577e-02 2.98892075e-02\n",
            " 3.57394146e-02 2.32599768e-02 2.91470946e-02 2.36427266e-02\n",
            " 3.52330581e-02 1.84383691e-02 5.26545211e-02 1.79892391e-02\n",
            " 1.72263381e-02 1.72543312e-02 1.81036919e-02 1.74350462e-02\n",
            " 1.20763723e-02 1.23203967e-02 1.71607845e-02 1.16206869e-02\n",
            " 3.52663663e-02 1.81574340e-02 2.33002538e-02 2.39523635e-02\n",
            " 1.71900769e-02 2.36322734e-02 2.95913229e-02 1.75673344e-02\n",
            " 1.80160510e-02 1.82408818e-02 1.68923105e-02 1.19644589e-02\n",
            " 1.15436172e-02 1.77867910e-02 1.77445060e-02 1.26077100e-02\n",
            " 1.10866326e-02 2.35133913e-02 1.77231863e-02 1.78624432e-02\n",
            " 1.75474321e-02 1.77212375e-02 1.77823617e-02 2.34605941e-02\n",
            " 2.93448181e-02 3.56729161e-02 4.68664422e-02 7.08991236e-02\n",
            " 7.05085192e-01 1.47372539e-01 4.15135559e-02 3.54075130e-02\n",
            " 3.53046354e-02 4.69630008e-02 4.16508049e-02 5.83623412e-02\n",
            " 2.33574212e-02 1.86803856e-02 2.95794524e-02 2.92141836e-02\n",
            " 2.36930433e-02 1.71857067e-02 1.22348819e-02 4.07474658e-02\n",
            " 1.76869252e-02 1.19078230e-02 1.83038959e-02 1.69120356e-02\n",
            " 2.97778847e-02 2.32011558e-02 4.69890450e-02 1.78095280e-02\n",
            " 1.17907716e-02 1.86582982e-02 2.27568094e-02 1.76102099e-02\n",
            " 2.38100356e-02 4.76990308e-02 4.76017045e-02 2.30606587e-02\n",
            " 1.73486455e-02 5.88535791e-02 2.36284347e-02 2.41434726e-02\n",
            " 2.34604170e-02 8.78260416e-02 5.30538660e-02 4.74701841e-02\n",
            " 8.82406822e-02 1.00000000e+00 2.39835457e-02 3.49266102e-02\n",
            " 3.53501685e-02 1.76233206e-02 2.42097348e-02 1.69110316e-02\n",
            " 2.96425846e-02 1.77507661e-02 2.36515851e-02 2.41833363e-02\n",
            " 2.36564869e-02 6.46692377e-02 2.33293100e-02 1.81253659e-02\n",
            " 4.05558842e-02 2.42875132e-02 2.37392851e-02 2.34958513e-02\n",
            " 1.72727570e-02 2.34451211e-02 1.84650630e-02 1.75486723e-02\n",
            " 1.67868934e-02 3.55151743e-02 1.76194228e-02 1.77407263e-02\n",
            " 1.17193715e-02 2.37095203e-02 2.94227146e-02 2.35544951e-02\n",
            " 1.88044648e-02 2.27672035e-02 2.37692271e-02 1.76094422e-02\n",
            " 4.16883062e-02 2.91205189e-02 1.19439070e-02 1.72383857e-02\n",
            " 2.93866897e-02 1.17705741e-02 3.52348888e-02 2.94629326e-02\n",
            " 1.78563013e-02 4.70642839e-02 3.53981229e-02 1.77087173e-02]\n",
            "(360,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFccA1epdfwZ",
        "colab_type": "code",
        "outputId": "50383188-822b-4579-9554-465fe39697e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#training configuration\n",
        "\n",
        "print(\"loading settings\")\n",
        "\n",
        "    \n",
        "learning_rate = 0.0001\n",
        "lr = learning_rate\n",
        "batch_size = 10\n",
        "#cond = 0\n",
        "latent_dim = 20 #20\n",
        "num_epochs = 50 #50\n",
        "vis_freq = 2\n",
        "labels = None \n",
        "D_rounds = 3\n",
        "G_rounds = 1\n",
        "hidden_units_g = 30  ##150\n",
        "hidden_units_d = 30   ##150\n",
        "num_generated_features = 2\n",
        "\n",
        "tf.reset_default_graph()\n",
        "\n",
        "CG = tf.placeholder(tf.float32, [batch_size, seq_length,num_generated_features]) #Placeholder 0 (shape: (50,253)) is it 0 or seq_length\n",
        "CD = tf.placeholder(tf.float32, [batch_size, seq_length,num_generated_features]) #Placerholder 1\n",
        "Z = tf.placeholder(tf.float32, [batch_size, seq_length, latent_dim]) #Placeholder 2\n",
        "W_out_G = tf.Variable(tf.truncated_normal([hidden_units_g, num_generated_features]))\n",
        "b_out_G = tf.Variable(tf.truncated_normal([num_generated_features]))\n",
        "\n",
        "X = tf.placeholder(tf.float32, [batch_size, seq_length, num_generated_features])\n",
        "# W_out_D = tf.Variable(tf.truncated_normal([hidden_units_d,1]))\n",
        "# b_out_D = tf.Variable(tf.truncated_normal([1]))\n",
        "W_out_D = tf.Variable(tf.truncated_normal([hidden_units_d,num_generated_features]))\n",
        "b_out_D = tf.Variable(tf.truncated_normal([num_generated_features]))\n",
        "# W_out_D = tf.Variable(tf.truncated_normal([hidden_units_d,1]))\n",
        "# b_out_D = tf.Variable(tf.truncated_normal([1]))\n",
        "\n",
        "\n",
        "print(\"settings loaded\")\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading settings\n",
            "settings loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T55YY029eEEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- to do with latent space --- #\n",
        "\n",
        "def sample_Z(batch_size, seq_length, latent_dim):\n",
        "    sample = np.float32(np.random.normal(size=[batch_size, seq_length, latent_dim]))\n",
        "    return sample\n",
        "\n",
        "\n",
        "\n",
        "#Define the generator and the discsriminator networks using LSTM cells\n",
        "\n",
        "def generator(z, c):\n",
        "    with tf.variable_scope(\"generator\",reuse=tf.AUTO_REUSE) as scope:\n",
        "        \n",
        "        # each step of the generator takes a random seed + the conditional embedding\n",
        "        print(\"AAAAAAAAAAAAAAA\")\n",
        "        print(z.shape)\n",
        "        print(c.shape)\n",
        "        c=z[:,:,0:21]\n",
        "#         print(d.shape)\n",
        "#         c=d\n",
        "        \n",
        "#         repeated_encoding = tf.tile(c,[1, tf.shape(z)[1]])\n",
        "# #         print(repeated_encoding.shape)\n",
        "#         repeated_encoding = tf.reshape(repeated_encoding, [tf.shape(z)[0], tf.shape(z)[1],10]) #SHOULD BE 0 INSTEAD OF 253 BUT THE ERROR\n",
        "# #         print(repeated_encoding.shape)\n",
        "#         generator_input = tf.concat([repeated_encoding, z], 2)\n",
        "#         print(generator_input.shape)\n",
        "        \n",
        "        generator_input = c\n",
        "        cell = tf.contrib.rnn.LSTMCell(num_units=hidden_units_g, state_is_tuple=True)\n",
        "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
        "                cell=cell,\n",
        "                dtype=tf.float32,\n",
        "                sequence_length=[seq_length]*batch_size,\n",
        "                inputs=generator_input)\n",
        "#         print(rnn_outputs)\n",
        "#         print(rnn_states)\n",
        "        rnn_outputs_2d = tf.reshape(rnn_outputs, [-1, hidden_units_g])\n",
        "        print(rnn_outputs_2d.shape)\n",
        "        logits_2d = tf.matmul(rnn_outputs_2d, W_out_G) + b_out_G\n",
        "        print(logits_2d.shape)\n",
        "        # output_2d = logits_2d\n",
        "#         output_2d = tf.identity(logits_2d)\n",
        "        output_2d = tf.nn.sigmoid(logits_2d)\n",
        "        # output_2d = tf.nn.tanh(logits_2d)\n",
        "        # output_2d = tf.nn.relu(logits_2d)\n",
        "        print(output_2d.shape)\n",
        "        output_3d = tf.reshape(output_2d, [-1, seq_length, num_generated_features])\n",
        "        print(output_3d.shape)\n",
        "        print(\"XXXXXXXXXX\")\n",
        "    return output_3d\n",
        "\n",
        "\n",
        "def discriminator(x, c, reuse=False):\n",
        "    with tf.variable_scope(\"discriminator\",reuse=tf.AUTO_REUSE) as scope:\n",
        "        # correct?\n",
        "        if reuse:\n",
        "            scope.reuse_variables()\n",
        "             \n",
        "        # each step of the generator takes one time step of the signal to evaluate + \n",
        "        # its conditional embedding  \n",
        "        print(\"BBBBBBBBBBB\")\n",
        "        print(x.shape)\n",
        "        print(c.shape)\n",
        "        repeated_encoding1 = tf.tile(x,[1,1, tf.shape(x)[1]])\n",
        "#         print(repeated_encoding1.shape)\n",
        "#         repeated_encoding = tf.tile(c,[1, tf.shape(x)[1]])\n",
        "#         repeated_encoding = tf.reshape(repeated_encoding, [tf.shape(x)[0], tf.shape(x)[1],10]) #SHOULD BE 0 INSTEAD OF 253 BUT THE ERROR\n",
        "        decoder_input = tf.concat([repeated_encoding1, x], 2)\n",
        "        print(decoder_input.shape)\n",
        "        cell = tf.contrib.rnn.LSTMCell(num_units=hidden_units_d, state_is_tuple=True)\n",
        "        rnn_outputs, rnn_states = tf.nn.dynamic_rnn(\n",
        "                cell=cell,\n",
        "                dtype=tf.float32,\n",
        "                inputs=decoder_input)\n",
        "#         print(rnn_outputs.shape,rnn_states.shape)\n",
        "        rnn_outputs_flat = tf.reshape(rnn_outputs, [-1, hidden_units_g])\n",
        "        print(rnn_outputs_flat.shape)\n",
        "        logits = tf.matmul(rnn_outputs_flat, W_out_D) + b_out_D\n",
        "        print(logits.shape)\n",
        "        output = tf.nn.sigmoid(logits)\n",
        "        print(output.shape)\n",
        "    return output, logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9TWwN3bwi4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# G_sample = generator(Z, CG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j1bof-bfJzu",
        "colab_type": "code",
        "outputId": "32fa3ee2-8075-4a78-f88a-23ca9611ec86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "# Define the loss funtion for the GAN\n",
        "\n",
        "# tf.reset_default_graph()\n",
        "G_sample = generator(Z, CG)\n",
        "D_real, D_logit_real = discriminator(X, CD)\n",
        "D_fake, D_logit_fake = discriminator(G_sample, CG, reuse=True)\n",
        "\n",
        "generator_vars = [v for v in tf.trainable_variables() if v.name.startswith('generator')]\n",
        "discriminator_vars = [v for v in tf.trainable_variables() if v.name.startswith('discriminator')]\n",
        "\n",
        "D_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_real,\n",
        "                                                                     labels=tf.ones_like(D_logit_real)))\n",
        "D_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake,\n",
        "                                                                     labels=tf.zeros_like(D_logit_fake)))\n",
        "D_loss = D_loss_real + D_loss_fake\n",
        "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_fake,\n",
        "                                                                labels=tf.ones_like(D_logit_fake)))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AAAAAAAAAAAAAAA\n",
            "(10, 360, 20)\n",
            "(10, 360, 2)\n",
            "(3600, 30)\n",
            "(3600, 2)\n",
            "(3600, 2)\n",
            "(10, 360, 2)\n",
            "XXXXXXXXXX\n",
            "BBBBBBBBBBB\n",
            "(10, 360, 2)\n",
            "(10, 360, 2)\n",
            "(10, 360, 722)\n",
            "(3600, 30)\n",
            "(3600, 2)\n",
            "(3600, 2)\n",
            "BBBBBBBBBBB\n",
            "(10, 360, 2)\n",
            "(10, 360, 2)\n",
            "(10, 360, 722)\n",
            "(3600, 30)\n",
            "(3600, 2)\n",
            "(3600, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFg6Vdn_eKb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the optimisers (GD for the discriminator and Adam for generator)\n",
        "    \n",
        "D_solver = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(D_loss, var_list=discriminator_vars)\n",
        "#D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=generator_vars)\n",
        "#G_solver = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(G_loss, var_list=discriminator_vars)\n",
        "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=generator_vars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Su-LQvHqeQQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################################################################################\n",
        "\n",
        "# Train the GAN on the Monte Carlo simulations\n",
        "\n",
        "# Starting a tensorflow session\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "vis_Z = sample_Z(batch_size, seq_length, latent_dim)\n",
        "\n",
        "\n",
        "t0 = time()\n",
        "\n",
        "def train_generator(batch_idx, offset):\n",
        "    # update the generator\n",
        "    for g in range(G_rounds):\n",
        "        #X_mb = get_batch(samples, batch_size, batch_idx + g + offset)\n",
        "        Y_mb = get_batch(samples, batch_size, batch_idx + g + offset)\n",
        "        _, G_loss_curr = sess.run([G_solver, G_loss],\n",
        "                                  feed_dict={CG:Y_mb,\n",
        "                                             Z: sample_Z(batch_size, seq_length, latent_dim)})\n",
        "        \n",
        "#         print(\"Size samples\")\n",
        "#         print(samples.shape)\n",
        "#         print(batch_size)\n",
        "#         print(batch_idx + g + offset)\n",
        "#         print(Y_mb.shape)\n",
        "# #         print(\"BBBBBBBBB\")\n",
        "# #         print(batch_size)\n",
        "# #         print(batch_idx + g + offset)\n",
        "#         print(\"ZZZZZZZZZZZZZZZZZZZZZZZZZZ\")\n",
        "    return G_loss_curr\n",
        "\n",
        "\n",
        "def train_discriminator(batch_idx, offset):\n",
        "    # update the discriminator\n",
        "    for d in range(D_rounds):\n",
        "    # using same input sequence for both the synthetic data and the real one,\n",
        "    # probably it is not a good idea...\n",
        "        X_mb = get_batch(samples, batch_size, batch_idx + d + offset)\n",
        "#         X_mb = X_mb.reshape(batch_size,seq_length,1)\n",
        "        Y_mb = get_batch(samples, batch_size, batch_idx + d + offset)\n",
        "        _, D_loss_curr = sess.run([D_solver, D_loss],\n",
        "                                  feed_dict={CD:Y_mb, CG:Y_mb, X:X_mb, \n",
        "                                             Z: sample_Z(batch_size, seq_length, latent_dim)})\n",
        "#         print(\"Size samples\")\n",
        "#         print(samples.shape)\n",
        "#         print(batch_size)\n",
        "#         print(batch_idx + d + offset)\n",
        "#         print(X_mb.shape)\n",
        "#         print(Y_mb.shape)\n",
        "#         print(\"XXXXXXXXXXXXXXXXXXXXXXXXXXX\")\n",
        "    return D_loss_curr\n",
        "\n",
        "\n",
        "# Functions used to visualize generated data\n",
        "def visualise_at_epoch(vis_sample, epoch, num_epochs):\n",
        "\n",
        "    save_plot_sample(vis_sample, epoch, n_samples=6,\n",
        "                     num_epochs=num_epochs) \n",
        "    return True\n",
        "\n",
        "def save_plot_sample(samples, idx, n_samples=6, num_epochs=None, ncol=2):\n",
        "    assert n_samples <= samples.shape[0]\n",
        "    assert n_samples % ncol == 0\n",
        "    sample_length = samples.shape[1]\n",
        "  \n",
        "    if not num_epochs is None:\n",
        "        col = hsv_to_rgb((1, 1.0*(idx)/num_epochs, 0.8))\n",
        "    else:\n",
        "        col = 'grey'\n",
        "\n",
        "    x_points = np.arange(sample_length)\n",
        "\n",
        "    nrow = int(n_samples/ncol)\n",
        "    fig, axarr = plt.subplots(nrow, ncol, sharex=True, figsize=(6, 6))\n",
        "    for m in range(nrow):\n",
        "        for n in range(ncol):\n",
        "            # first column\n",
        "            sample = samples[n*nrow + m, :, 0]\n",
        "            axarr[m, n].plot(x_points, sample, color=col)\n",
        "            axarr[m, n].set_ylim(-1, 1)\n",
        "    for n in range(ncol):\n",
        "        axarr[-1, n].xaxis.set_ticks(range(0, sample_length, int(sample_length/4)))\n",
        "    fig.suptitle(idx)\n",
        "    fig.subplots_adjust(hspace = 0.15)\n",
        "    plt.clf()\n",
        "    plt.close()\n",
        "    return\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wFxD2O6eV6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# --- train --- #\n",
        "\n",
        "print('num_epoch\\t D_loss_curr\\t G_loss_curr\\t time elapsed')  \n",
        "\n",
        "d_loss = []\n",
        "g_loss = []\n",
        "plot_series_real(0)\n",
        "for num_epoch in range(8000):\n",
        "    \n",
        "    for batch_idx in range(0, int(len(samples)/batch_size) - (D_rounds + G_rounds), D_rounds + G_rounds): \n",
        "        \n",
        "        if num_epoch % 2 == 0: \n",
        "            \n",
        "            G_loss_curr = train_generator(batch_idx,0)\n",
        "            D_loss_curr = train_discriminator(batch_idx, G_rounds)\n",
        "#             print('01',G_loss_curr,D_loss_curr)\n",
        "#             print(G_rounds)\n",
        "#             print(batch_idx)\n",
        "#             print('XXXXXXXXXXXXXXXXXX')\n",
        "#             plot_series_real(0)\n",
        "#             plot_series_gen(0)\n",
        "            \n",
        "        else: \n",
        "            \n",
        "            D_loss_curr = train_discriminator(batch_idx, 0)\n",
        "            G_loss_curr = train_generator(batch_idx, D_rounds)\n",
        "#             print('02',G_loss_curr,D_loss_curr)\n",
        "        \n",
        "#         print(len(samples))\n",
        "        d_loss.append(D_loss_curr)\n",
        "        g_loss.append(G_loss_curr)\n",
        "        t = time() -t0\n",
        "        if batch_idx % 100 == 0:\n",
        "          print(num_epoch,'\\t', D_loss_curr, '\\t', G_loss_curr, '\\t', t)        \n",
        "        \n",
        "    # save synthetic data\n",
        "    if num_epoch % 5 == 0:\n",
        "    # generate synthetic dataset\n",
        "        gen_samples = []\n",
        "#         print(len(samples))\n",
        "        for batch_idx in range(int(len(samples) / batch_size)):\n",
        "            X_mb = get_batch(samples, batch_size, batch_idx)\n",
        "            Y_mb = get_batch(samples, batch_size, batch_idx)\n",
        "            z_ = sample_Z(batch_size, seq_length, latent_dim)\n",
        "            gen_samples_mb = sess.run(G_sample, feed_dict={Z: z_, CG:Y_mb})\n",
        "            gen_samples.append(gen_samples_mb)\n",
        "#             print (batch_idx)\n",
        "        \n",
        "        gen_samples = np.vstack(gen_samples)\n",
        "        #Plot\n",
        "        print(gen_samples.shape)\n",
        "    if num_epoch % 20 == 0:\n",
        "        plot_series_gen(0)\n",
        "\n",
        "\n",
        "\n",
        "ax = pd.DataFrame(\n",
        "    {\n",
        "        'Generative Loss': g_loss,\n",
        "        'Discriminative Loss': d_loss,\n",
        "    }\n",
        ").plot(title='Training loss', logy=True)\n",
        "ax.set_xlabel(\"Training iterations\")\n",
        "ax.set_ylabel(\"Loss\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4TuNIkH3hW2",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-oCOHHTpfUN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython import display\n",
        "from google.colab import files\n",
        "\n",
        "# # Plot the price evolution of the generated sample\n",
        "# def plot_price_gen(num_gen_sample = 1): \n",
        "    \n",
        "#     for i in range(num_gen_sample): \n",
        "#         daily_log_returns = generated_data[:,:,i]\n",
        "        \n",
        "#         price_list = daily_log_returns\n",
        "        \n",
        "# #         for x in daily_log_returns: \n",
        "# #             price_list.append(price_list[-1]*np.exp(x))\n",
        "            \n",
        "        \n",
        "#         plt.figure(0)\n",
        "#         plt.plot(price_list[0,:],price_list[1,:])\n",
        "#         plt.figure(1)\n",
        "#         plt.plot(range(price_list.shape[1]),price_list[1,:])\n",
        "#         plt.figure(2)\n",
        "#         plt.plot(range(price_list.shape[1]),price_list[0,:])\n",
        "        \n",
        "# #         plt.scatteprice_listr(daily_log_returns[price_list])\n",
        "        \n",
        "#         plt.show()\n",
        "#         display.display(plt.gcf())\n",
        "        \n",
        "#     return 0\n",
        "\n",
        "# def plot_series_real(num_gen_samples = 1):\n",
        "# #         samples=oldsamples[:,:,1]\n",
        "#         fig = plt.figure(figsize=(45, 5))\n",
        "#         plt.axis('off')\n",
        "#         fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "#         for i in range(0, 6):\n",
        "#           ax = fig.add_subplot(3, 2 , i+1)\n",
        "# #           a=images[i-1,:]\n",
        "# #           c=a.reshape([2,360]).numpy()\n",
        "# #           print(c.shape)\n",
        "#           lists=samples[2*i,:]\n",
        "# #           ax.scatter(range(lists.shape[0]),lists,marker='.',color='blue')\n",
        "#           ax.plot(range(lists.shape[0]),lists, color='tab:blue', marker='o')\n",
        "#           ax.plot(range(lists.shape[0]),lists, color='black')\n",
        "\n",
        "def plot_series_gen(num_gen_samples = 1):\n",
        "#         generated_data = np.transpose(gen_samples)\n",
        "        generated_data=gen_samples[:,:,1].reshape([100,360])\n",
        "        generated_data0=gen_samples[:,:,0].reshape([100,360])\n",
        "        generated_data0=np.cumsum(generated_data0,axis=1) \n",
        "        \n",
        "#         samples=generated_data[:,:,i]\n",
        "        fig = plt.figure(figsize=(45, 5))\n",
        "        plt.axis('off')\n",
        "        fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "        for i in range(0, 4):\n",
        "          ax = fig.add_subplot(2, 2 , i+1)\n",
        "#           a=images[i-1,:]\n",
        "#           c=a.reshape([2,360]).numpy()\n",
        "#           print(c.shape)\n",
        "          lists0=generated_data0[i,:]\n",
        "          lists=generated_data[i,:]\n",
        "#           ax.scatter(range(lists.shape[0]),lists,marker='.',color='blue')\n",
        "          ax.plot(lists0,lists, color='tab:blue', marker='o')\n",
        "          ax.plot(lists0,lists, color='black')\n",
        "        display.display(plt.gcf())\n",
        "\n",
        "def plot_oldseries_real(num_gen_samples = 1):\n",
        "        # samples1=nsamples[:,:,0]\n",
        "        samples1=oldsamples[:,:,1]\n",
        "        fig = plt.figure(figsize=(45, 250))\n",
        "        # fig = plt.figure()\n",
        "        plt.axis('off')\n",
        "        fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "        for i in range(0, 100):\n",
        "          ax = fig.add_subplot(50, 2 , i+1)\n",
        "#           a=images[i-1,:]\n",
        "#           c=a.reshape([2,360]).numpy()\n",
        "#           print(c.shape)\n",
        "          lists=samples1[i,:]\n",
        "\n",
        "#           ax.scatter(range(lists.shape[0]),lists,marker='.',color='blue')\n",
        "          ax.plot(range(lists.shape[0]),lists, color='tab:blue', marker='o')\n",
        "          ax.plot(range(lists.shape[0]),lists, color='black')\n",
        "        display.display(plt.gcf())\n",
        "        plt.savefig(\"abc.png\")\n",
        "        files.download(\"abc.png\")\n",
        "    \n",
        "def plot_series_real(num_gen_samples = 1):\n",
        "        samples1=nsamples[:,:,1]\n",
        "        samples0=nsamples[:,:,0]\n",
        "        samples0=np.cumsum(samples0,axis=1)\n",
        "        # samples1=oldsamples[:,:,1]\n",
        "        fig = plt.figure(figsize=(45, 5))\n",
        "        # fig = plt.figure()\n",
        "        plt.axis('off')\n",
        "        fig.subplots_adjust(hspace=0.2, wspace=0.2)\n",
        "        for i in range(0, 4):\n",
        "          ax = fig.add_subplot(2, 2 , i+1)\n",
        "#           a=images[i-1,:]\n",
        "#           c=a.reshape([2,360]).numpy()\n",
        "#           print(c.shape)\n",
        "          lists0=samples0[2*i,:]\n",
        "          lists=samples1[2*i,:]\n",
        "#           ax.scatter(range(lists.shape[0]),lists,marker='.',color='blue')\n",
        "          ax.plot(lists0,lists, color='tab:blue', marker='o')\n",
        "          ax.plot(lists0,lists, color='black')\n",
        "        display.display(plt.gcf())\n",
        "    \n",
        "# def plot_serie_real(num_gen_samples = 1):\n",
        "#     for i in range(num_gen_samples):\n",
        "#       plt.figure(i)\n",
        "#       lists=samples[i,:]\n",
        "# #       print(lists)\n",
        "# #       print(lists.shape)\n",
        "# # plt.subplot(211)\n",
        "#       plt.plot(range(lists.shape[0]),lists, color='tab:blue', marker='o')\n",
        "# #       plt.scatter(range(lists.shape[0]),lists)\n",
        "      \n",
        "# Plot one real evolution\n",
        "def plot_price_real(num_gen_samples = 1): \n",
        "    \n",
        "    for i in range(num_gen_samples):\n",
        "        #create list of daily returns using random normal distribution\n",
        "        mu1=mu/T\n",
        "        sigma1=vol/math.sqrt(4*T)\n",
        "        daily_returns = scy.stats.truncnorm.rvs((-0.99 - mu1) / sigma1, (2 - mu1) / sigma1,loc=mu/T,scale=vol/math.sqrt(4*T),size=seq_length)\n",
        "#         daily_returns=np.random.normal(mu/T,vol/math.sqrt(T),T)+1\n",
        " \n",
        "        #set starting price and create price series generated by above random daily returns\n",
        "        price_list = [S]\n",
        " \n",
        "        for x in daily_returns:\n",
        "            price_list.append(price_list[-1]*x)\n",
        "        \n",
        "        #plot data from each individual run which we will plot at the end\n",
        "        plt.plot(price_list)\n",
        "        #pd.DataFrame(price_list).plot() #(if we want plots on separate figures)\n",
        "    \n",
        "    return plt.show() # return None (if we want plots on separate figures)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s85YxUhJp4lB",
        "colab_type": "code",
        "outputId": "a3d2fe36-e66b-4e06-b7a9-3f0d41e19f4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# generated_data = np.transpose(gen_samples)\n",
        "generated_data = gen_samples\n",
        "generated_data = generated_data.reshape([100,360])\n",
        "print(generated_data.shape)\n",
        " \n",
        "# plot_price_gen(20) \n",
        "# plot_price_real(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 360)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAKCwYzPv_E7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_oldseries_real(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KoLkdEHY5dc",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLjy6rNgR2UW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_series_gen(50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWM3ZeHJG72P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(oldsamples[1,:,1])\n",
        "print(oldsamples[2,:,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pI0RnEgwIs4",
        "colab_type": "code",
        "outputId": "36c94f92-3684-42ce-f1b5-77ba70d0bcf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(oldsamples.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(100, 360, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrCG4FBhfNMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generated_data = np.transpose(gen_samples)\n",
        "# #plot the log-returns\n",
        "# gen_ind = 1 # change in function price as well\n",
        "# pd.DataFrame(generated_data[0,:,gen_ind]).plot()  # 1 is the index of the plotted sample out of the 1000 generated\n",
        "\n",
        "# get the prices from the log returns\n",
        "def price_gen(ind_gen_sample = 1): \n",
        "    \n",
        "    daily_log_returns=generated_data[0,:,ind_gen_sample]\n",
        "    \n",
        "    price_list = [S] #initial price (today)\n",
        "\n",
        "    for x in np.transpose(daily_log_returns): \n",
        "\n",
        "        price_list.append(price_list[-1]*np.exp(x))\n",
        "        \n",
        "    #price_list = np.asarray(price_list)\n",
        "    \n",
        "    return price_list\n",
        "\n",
        "def price_real(): \n",
        "    \n",
        "    daily_returns=np.random.normal(mu/T,vol/math.sqrt(T),T)+1\n",
        "    \n",
        "    price_list = [S]\n",
        " \n",
        "    for x in daily_returns:\n",
        "        \n",
        "        price_list.append(price_list[-1]*x)\n",
        "        \n",
        "    return price_list\n",
        "\n",
        "\n",
        "def prices_gen_data_frame(num_samples = 1): \n",
        "    df = pd.DataFrame([])\n",
        "    \n",
        "    for i in range(num_samples): \n",
        "        \n",
        "        df[i] = price_real()\n",
        "    \n",
        "    return df\n",
        "\n",
        "def prices_real_data_frame(num_samples = 1): \n",
        "    df = pd.DataFrame()\n",
        "    \n",
        "    for i in range(num_samples): \n",
        "        \n",
        "        df[i] = price_real()\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Plot the price evolution of the generated sample\n",
        "def plot_price_gen(num_gen_sample = 1): \n",
        "    \n",
        "    for i in range(num_gen_sample): \n",
        "        daily_log_returns = generated_data[0,:,i]\n",
        "        \n",
        "        price_list = [S]\n",
        "        \n",
        "        for x in daily_log_returns: \n",
        "            price_list.append(price_list[-1]*np.exp(x))\n",
        "        \n",
        "        plt.plot(price_list)\n",
        "        \n",
        "    return plt.show()\n",
        "\n",
        "# Plot one real evolution\n",
        "def plot_price_real(num_gen_samples = 1): \n",
        "    \n",
        "    for i in range(num_gen_samples):\n",
        "        #create list of daily returns using random normal distribution\n",
        "        mu1=mu/T\n",
        "        sigma1=vol/math.sqrt(4*T)\n",
        "        daily_returns = scy.stats.truncnorm.rvs((-0.99 - mu1) / sigma1, (2 - mu1) / sigma1,loc=mu/T,scale=vol/math.sqrt(4*T),size=seq_length)\n",
        "#         daily_returns=np.random.normal(mu/T,vol/math.sqrt(T),T)+1\n",
        " \n",
        "        #set starting price and create price series generated by above random daily returns\n",
        "        price_list = [S]\n",
        " \n",
        "        for x in daily_returns:\n",
        "            price_list.append(price_list[-1]*x)\n",
        "        \n",
        "        #plot data from each individual run which we will plot at the end\n",
        "        plt.plot(price_list)\n",
        "        #pd.DataFrame(price_list).plot() #(if we want plots on separate figures)\n",
        "    \n",
        "    return plt.show() # return None (if we want plots on separate figures)\n",
        "\n",
        "\n",
        "#Statistical tests on the final prices\n",
        "\n",
        "def transform2darray(vector):\n",
        "    v = []\n",
        "    for i in range(len(vector)):\n",
        "        v.append(vector[i][0])\n",
        "    return v\n",
        "\n",
        "# num_sample = 50000\n",
        "# result_gen = prices_gen_data_frame(num_sample).iloc[[seq_length-1]]\n",
        "# result_gen = np.asarray(result_gen)\n",
        "# result_gen = np.transpose(result_gen)\n",
        "# result_gen = transform2darray(result_gen)\n",
        "# result_real = prices_real_data_frame(num_sample).iloc[[seq_length-1]]\n",
        "# result_real = np.asarray(result_real)\n",
        "# result_real = np.transpose(result_real)\n",
        "# result_real = transform2darray(result_real)\n",
        "# results = pd.DataFrame({'Real':result_real, 'Fake': result_gen})\n",
        "\n",
        "# # statistics summary\n",
        "# print(results.describe())\n",
        "# # boxplot\n",
        "# results.boxplot()\n",
        "\n",
        "# results.hist(bins=100)\n",
        "\n",
        "# # Komogorov-Smirnov (This is a two-sided test for the null hypothesis that 2 independent samples are drawn from the same continuous distribution.)\n",
        "# # calculate the significance\n",
        "# value, pvalue = ks_2samp(result_real, result_gen)\n",
        "# print(\"test statistic =\", value, \"  \", \"pvalue = \", pvalue)\n",
        "# if pvalue > 0.05:\n",
        "# \tprint('Samples are likely drawn from the same distributions (not reject H0)')\n",
        "# else:\n",
        "# \tprint('Samples are likely drawn from different distributions (reject H0)')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}